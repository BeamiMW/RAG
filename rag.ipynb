{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq #model llm, bisa open ai, gemini, bison, open llama, groq, nvidia. kl rag, disarankan pake API yg open source\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings #pake embedding, membuat vektor yg akan disimpan dlm vector db\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain #u/ processing text nya, next level ada namanya textsplitter, etc\n",
    "from langchain_core.prompts import ChatPromptTemplate #prompt template krn kasus spesifik\n",
    "from langchain.chains import create_retrieval_chain #chain retrieval , dari user input sampai jd output\n",
    "from langchain_community.vectorstores import FAISS #chromadb, astradb, cosmosdb, FAISS (vector db)\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader #karena datanya pdf yg akan diload\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi model\n",
    "def init_groq():\n",
    "    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not groq_api_key:\n",
    "        print(\"‚ùå GROQ_API_KEY not found!\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return ChatGroq(\n",
    "            groq_api_key=groq_api_key,\n",
    "            model_name=\"mixtral-8x7b-32768\",\n",
    "            temperature=0,\n",
    "            max_tokens=4000 #pembatasan karakter yg akan dijadikan jawaban 4000 * 0.005*30*24 (biayanya)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Groq: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Initialize Groq\n",
    "llm = init_groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_embedding():\n",
    "    try:\n",
    "        print(\"Creating vector embeddings...\")\n",
    "        \n",
    "        # periksa apakah directory sudah ada ?\n",
    "        if not os.path.exists(\"kuhp\"): #nama folder\n",
    "            os.makedirs(\"kuhp\",exist_ok=True)\n",
    "            print(\"üìÅ Created 'kuhp' directory. Please add your PDF files there.\")\n",
    "            return None, None\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name= \"all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'}, #kl punya gpu, pake gpu\n",
    "            encode_kwargs={\n",
    "                'normalize_embeddings': True, #dijadikan 0-1\n",
    "                'batch_size': 32 #per batch akan memasukkan 32 data (bisa di tunning/diubah2)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Load documents\n",
    "        loader = PyPDFDirectoryLoader(\"kuhp\")\n",
    "        docs = loader.load()\n",
    "        \n",
    "        if not docs:\n",
    "            print(\"‚ùå No documents found in directory!\")\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"Found {len(docs)} documents\")\n",
    "        \n",
    "        # Split documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, #1000 karakter. chunk kl misal di doc tersbut ada kata \"saya suka makan nasi goreng di psar minggu\" berarti itu 1 chunk\n",
    "            chunk_overlap=100 #100 karakter sebelumnya. \n",
    "        ) #chunk untuk tunning. \n",
    "        final_documents = text_splitter.split_documents(docs) #load data ke dalam vektor db\n",
    "        \n",
    "        print(f\"Created {len(final_documents)} document chunks\")\n",
    "        \n",
    "        # membuat vector store, dimasukkan ke dalam vectornya. vectornya kita pake faiss\n",
    "        vectors = FAISS.from_documents(\n",
    "            final_documents,\n",
    "            embeddings\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Vector store created successfully!\")\n",
    "        return vectors, docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during embedding creation: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant for analyzing legal documents in PDF format. Answer the questions based solely on the provided context.\n",
    "Focus on providing accurate, clear, and detailed responses specific to Indonesian legal terminology and structure.\n",
    "Additionally, if a part of the document is ambiguous, explain your reasoning for any assumptions made.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_prompt, vectors, llm, prompt):\n",
    "    # periksa apakah vector dan llm sudah ada\n",
    "    if vectors is None:\n",
    "        print(\"‚ùå Vector store not initialized! Please create embeddings first.\")\n",
    "        return None\n",
    "    \n",
    "    if llm is None:\n",
    "        print(\"‚ùå Language model not initialized! Please check your GROQ_API_KEY.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Create chains\n",
    "        document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "        retriever = vectors.as_retriever(\n",
    "            search_kwargs={\"k\": 10} #ambil 10 yang similar\n",
    "        )\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        \n",
    "        # Get response\n",
    "        start = time.process_time() #u/ itung waktu\n",
    "        response = retrieval_chain.invoke({'input': user_prompt})\n",
    "        response_time = time.process_time() - start\n",
    "        \n",
    "        print(f\"‚è±Ô∏è Response time: {response_time:.2f} seconds\")\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(response['answer'])\n",
    "        \n",
    "        print(\"\\nRelated Document Excerpts:\")\n",
    "        for i, doc in enumerate(response['context']):\n",
    "            print(f\"\\nDocument {i+1}:\")\n",
    "            print(doc.page_content)\n",
    "            if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\") #kl tidak match maka unknown, chatbot akan ngomong dia tdk paham dgn yg akan dimaksud\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector embeddings...\n",
      "Error during embedding creation: name 'SentenceTransformer' is not defined\n"
     ]
    }
   ],
   "source": [
    "vectors, docs = create_vector_embedding()\n",
    "\n",
    "# response = process_query(user_prompt, vectors, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Vector store not initialized! Please create embeddings first.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"JELASKAN BAB 1 RUANG LINGKUP BERLAKUNYA KETENTUAN PERATURAN PERUNDANG UNDANGAN PIDANA Bagian Kesatu Menurut Waktu pada bagian pasal 1, jelaskan point pointnya\"\n",
    "response = process_query(user_prompt, vectors, llm, prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
